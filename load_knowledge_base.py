import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

def create_knowledge_base():
    """–°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ data/"""
    
    print("üîç –ù–∞—á–∏–Ω–∞—é —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ data
    if not os.path.exists('data'):
        print("‚ùå –ü–∞–ø–∫–∞ 'data' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print("–°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É 'data' –∏ –¥–æ–±–∞–≤—å—Ç–µ —Ç—É–¥–∞ .txt —Ñ–∞–π–ª—ã —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π")
        return False
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
    data_files = [f for f in os.listdir('data') if f.endswith('.txt')]
    
    if not data_files:
        print("‚ùå –í –ø–∞–ø–∫–µ 'data' –Ω–µ—Ç .txt —Ñ–∞–π–ª–æ–≤!")
        print("–î–æ–±–∞–≤—å—Ç–µ —Ñ–∞–π–ª—ã —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –≤ –ø–∞–ø–∫—É 'data/'")
        return False
    
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(data_files)}")
    
    try:
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings = HuggingFaceEmbeddings(
            model_name='sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
            model_kwargs={'device': 'cpu'}
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Ç–µ–∫—Å—Ç–∞
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        all_documents = []
        
        for filename in data_files:
            file_path = os.path.join('data', filename)
            print(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {filename}")
            
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
                loader = TextLoader(file_path, encoding='utf-8')
                documents = loader.load()
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                chunks = text_splitter.split_documents(documents)
                all_documents.extend(chunks)
                
                print(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {filename}: {e}")
                continue
        
        if not all_documents:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∏ –æ–¥–∏–Ω —Ñ–∞–π–ª!")
            return False
        
        print(f"üìä –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(all_documents)}")
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        print("üîÑ –°–æ–∑–¥–∞—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π...")
        vector_db = Chroma.from_documents(
            documents=all_documents,
            embedding=embeddings,
            persist_directory="./chroma_db"
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑—É
        vector_db.persist()
        
        print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!")
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(all_documents)}")
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: ./chroma_db/")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
        return False

if __name__ == "__main__":
    create_knowledge_base()
